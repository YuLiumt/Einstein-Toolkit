<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Yu Liu">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Numerical Methods - Yu Liu's Notebook</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Numerical Methods";
    var mkdocs_page_input_path = "NR/Numerical Methods.md";
    var mkdocs_page_url = "/Einstein-Toolkit/NR/Numerical Methods/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-36723568-3', 'https://yuliumt.github.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Yu Liu's Notebook</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Einstein Toolkit</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../ET/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Cactus/">Cactus</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Thorn/">Thorn</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Source File/">Source File</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/parameter/">Parameter</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Lorene/">Lorene</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Visualization/">Visualization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">General Relativity</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../GR/Differential geometry/">Differential geometry</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GR/Einstein Equation/">Einstein Equation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GR/Solutions/">Solutions</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gravitational Wave</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../GW/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GW/Linearized Waves/">Linearized Waves</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GW/Extracting Gravitational Waveforms/">Extracting Gravitational Waveforms</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Numerical Relativity</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../3+1 Decomposition/">3+1 Decomposition</a>
                </li>
                <li class="">
                    
    <a class="" href="../ADM/">ADM</a>
                </li>
                <li class="">
                    
    <a class="" href="../BSSN/">BSSN</a>
                </li>
                <li class="">
                    
    <a class="" href="../MHD/">MHD</a>
                </li>
                <li class="">
                    
    <a class="" href="../Conformal Transformation/">Conformal Transformation</a>
                </li>
                <li class="">
                    
    <a class="" href="../Coordinates/">Coordinates</a>
                </li>
                <li class="">
                    
    <a class="" href="../Matter Sources/">Matter Sources</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Numerical Methods</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#finite-difference-methods">Finite Difference Methods</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#elliptic-equations">Elliptic Equations</a></li>
        
            <li><a class="toctree-l4" href="#hyperbolic-equations">Hyperbolic Equations</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#ghost-zones">Ghost Zones</a></li>
    

    <li class="toctree-l3"><a href="#mesh-refinement">Mesh Refinement</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#time-evolution-scheme">Time evolution scheme</a></li>
        
            <li><a class="toctree-l4" href="#inter-grid-transport-operators">Inter-grid transport operators</a></li>
        
            <li><a class="toctree-l4" href="#initial-data-generation">Initial data generation</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#pseudo-spectral-methods">Pseudo-spectral methods</a></li>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Search Pipeline</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../SP/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../SP/matched filter/">Matched Filtering</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Example</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Example/Poisson/">Poisson</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Yu Liu's Notebook</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Numerical Relativity &raquo;</li>
        
      
    
    <li>Numerical Methods</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/YuLiumt/Einstein-Toolkit/edit/master/docs/NR/Numerical Methods.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>There are two major classes of techniques used for numerical simulations of the Einstein equations. These are <strong>finite-difference methods</strong>, typically coupled to adaptive mesh refinement techniques, and <strong>pseudo-spectral methods</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Most of the finite difference codes are based on modifications to the ADM system. These equations are in a form with mixed second and first derivatives. Basically, the system is such that <strong>only first time derivatives occur, but first and second spatial derivatives occur.</strong> (Of course, auxiliary evolution variables can be introduced so that the system only has first spatial derivatives, but at the cost of introducing additional constraints.)</p>
</div>
<p>A black hole interior presents a major challenge to any numerical technique because of the curvature singularity it harbors. Fortunately, the singularity is concealed behind an event horizon. The region inside the horizon cannot affect the exterior solution, so numerical simulations need not evolve it accurately.</p>
<p>One way to do this is to simply not evolve a region inside the horizon, i.e., to excise this region. The other method, the puncture method involves allowing singularities in the computational domain. In the appropriate gauge, these singularities are sufficiently benign that finite difference methods can handle them.</p>
<h3 id="finite-difference-methods">Finite Difference Methods</h3>
<p><strong>In a finite difference approximation a function <span><span class="MathJax_Preview">f(t,x)</span><script type="math/tex">f(t,x)</script></span> is represented by values at a discrete set of points.</strong> At the core of finite difference approximation is therefore a discretization of the spacetime, or a numerical grid. Instead of evaluating f at all values of x, for example, we only consider discrete values <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>. The distance between the gridpoints <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> is called the gridspacing <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>. For uniform grids, for which <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> is constant, we have</p>
<div>
<div class="MathJax_Preview">
x _ { i } = x _ { 0 } + i \Delta x
</div>
<script type="math/tex; mode=display">
x _ { i } = x _ { 0 } + i \Delta x
</script>
</div>
<p>If the solution depends on time we also discretize the time coordinate, for example as</p>
<div>
<div class="MathJax_Preview">
t ^ { n } = t ^ { 0 } + n \Delta t
</div>
<script type="math/tex; mode=display">
t ^ { n } = t ^ { 0 } + n \Delta t
</script>
</div>
<p>The finite difference representation of the function <span><span class="MathJax_Preview">f(t,x)</span><script type="math/tex">f(t,x)</script></span>, for example, is</p>
<div>
<div class="MathJax_Preview">
f _ { i } ^ { n } = f \left( t ^ { n } , x _ { i } \right) + \text { truncation error. }
</div>
<script type="math/tex; mode=display">
f _ { i } ^ { n } = f \left( t ^ { n } , x _ { i } \right) + \text { truncation error. }
</script>
</div>
<p>Differential equations involve derivatives, so we must next discuss how to represent derivatives in a finite difference representation.</p>
<p>Assuming that <span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> can be differentiated to sufficiently high order and that it can be represented as a Taylor series, we have</p>
<div>
<div class="MathJax_Preview">
f _ { i + 1 } = f \left( x _ { i } + \Delta x \right) = f \left( x _ { i } \right) + \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</div>
<script type="math/tex; mode=display">
f _ { i + 1 } = f \left( x _ { i } + \Delta x \right) = f \left( x _ { i } \right) + \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</script>
</div>
<p>Solving for <span><span class="MathJax_Preview">\left( \partial _ { x } f \right) _ { x _ { i } } = \left( \partial _ { x } f \right) _ { i }</span><script type="math/tex">\left( \partial _ { x } f \right) _ { x _ { i } } = \left( \partial _ { x } f \right) _ { i }</script></span> we find</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i } } { \Delta x } + \mathcal { O } ( \Delta x )
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i } } { \Delta x } + \mathcal { O } ( \Delta x )
</script>
</div>
<p>The truncation error of this expression is linear in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>, and it turns out that we can do better. We call equation a <strong>one-sided derivative</strong>, since it uses only neighbors on one side of <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>.</p>
<p>Consider the Taylor expansion to the point <span><span class="MathJax_Preview">x_{ i − 1 }</span><script type="math/tex">x_{ i − 1 }</script></span>,</p>
<div>
<div class="MathJax_Preview">
f _ { i - 1 } = f \left( x _ { i } - \Delta x \right) = f \left( x _ { i } \right) - \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</div>
<script type="math/tex; mode=display">
f _ { i - 1 } = f \left( x _ { i } - \Delta x \right) = f \left( x _ { i } \right) - \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</script>
</div>
<p>we now find</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i - 1 } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i - 1 } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</script>
</div>
<p>which is second order in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>. In general, <strong>centered derivatives</strong> lead to higher order schemes than one-sided derivatives for the same number of gridpoints.</p>
<p>The key point is that we are able to combine the two Taylor expansions in such a way that the leading order error term cancels out, leaving us <strong>with a higher order representation of the derivative</strong>. This cancellation <strong>only works out for uniform grids</strong>, when <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> is independent of x. <mark>This is one of the reasons why many current numerical relativity applications of finite difference schemes work with uniform grids.</mark></p>
<p>Higher order derivatives can be constructed in a similar fashion. Adding the two Taylor expansions all terms odd in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> drop out and we find for the second derivative</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { f _ { i + 1 } - 2 f _ { i } + f _ { i - 1 } } { ( \Delta x ) ^ { 2 } } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { f _ { i + 1 } - 2 f _ { i } + f _ { i - 1 } } { ( \Delta x ) ^ { 2 } } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</script>
</div>
<div class="admonition note">
<p class="admonition-title">Fourth-order finite-difference</p>
<p>Fourth-order finite-difference representations of the first and second derivatives of a function f are given by</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { 1 } { 12 \Delta x } \left( f _ { i - 2 } - 8 f _ { i - 1 } + 8 f _ { i + 1 } - f _ { i + 2 } \right) \\
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { 1 } { 12 ( \Delta x ) ^ { 2 } } \left( - f _ { i - 2 } + 16 f _ { i - 1 } - 30 f _ { i } + 16 f _ { i + 1 } - f _ { i + 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { 1 } { 12 \Delta x } \left( f _ { i - 2 } - 8 f _ { i - 1 } + 8 f _ { i + 1 } - f _ { i + 2 } \right) \\
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { 1 } { 12 ( \Delta x ) ^ { 2 } } \left( - f _ { i - 2 } + 16 f _ { i - 1 } - 30 f _ { i } + 16 f _ { i + 1 } - f _ { i + 2 } \right)
</script>
</div>
<p>where we have omitted the truncation error, <span><span class="MathJax_Preview">\mathcal { O } \left( \Delta x ^ { 4 } \right)</span><script type="math/tex">\mathcal { O } \left( \Delta x ^ { 4 } \right)</script></span></p>
</div>
<h4 id="elliptic-equations">Elliptic Equations</h4>
<p>As an example of a simple, one-dimensional elliptic equation consider</p>
<div>
<div class="MathJax_Preview">
\partial_{x}^{2} f=s
</div>
<script type="math/tex; mode=display">
\partial_{x}^{2} f=s
</script>
</div>
<p>We first have to construct a numerical grid that covers an interval between <span><span class="MathJax_Preview">x_{min}</span><script type="math/tex">x_{min}</script></span> and <span><span class="MathJax_Preview">x_{max}</span><script type="math/tex">x_{max}</script></span>. We then divide the interval <span><span class="MathJax_Preview">\left[x_{\min }, x_{\max }\right]</span><script type="math/tex">\left[x_{\min }, x_{\max }\right]</script></span> into N gridcells, leading to a gridspacing of</p>
<div>
<div class="MathJax_Preview">
\Delta x=\frac{x_{\max }-x_{\min }}{N}
</div>
<script type="math/tex; mode=display">
\Delta x=\frac{x_{\max }-x_{\min }}{N}
</script>
</div>
<p>We can choose our grid points to be located either at the center of these cells, which would be referred to as a cell-centered grid, or on the vertices, which would be referred to as a vertex-centered grid. For a cell-centered grid we have N grid points located at</p>
<div>
<div class="MathJax_Preview">
x_{i}=x_{\min }+(i-1 / 2) \Delta x, \quad i=1, \ldots, N
</div>
<script type="math/tex; mode=display">
x_{i}=x_{\min }+(i-1 / 2) \Delta x, \quad i=1, \ldots, N
</script>
</div>
<p><img alt="" src="../media/15516083713832.jpg" /></p>
<p>whereas for a vertex centered grid we have N + 1 gridpoints located a</p>
<div>
<div class="MathJax_Preview">
x_{i}=x_{\min }+(i-1) \Delta x, \quad i=1, \ldots, N+1
</div>
<script type="math/tex; mode=display">
x_{i}=x_{\min }+(i-1) \Delta x, \quad i=1, \ldots, N+1
</script>
</div>
<p><strong>The difference between cell-centered and vertex-centered grids only affects the implementation of boundary conditions, but not the finite difference representation of the differential equation itself.</strong></p>
<p>We are now ready to finite difference the differential equation. We define two arrays, <span><span class="MathJax_Preview">f_i</span><script type="math/tex">f_i</script></span> and <span><span class="MathJax_Preview">s_i</span><script type="math/tex">s_i</script></span>, which represent the functions f and s at the gridpoints <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> for <span><span class="MathJax_Preview">i = 1, . . . , N</span><script type="math/tex">i = 1, . . . , N</script></span>. In the interior of our domain we can represent the differential equation as</p>
<div>
<div class="MathJax_Preview">
f_{i+1}-2 f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=2, \ldots, N-1
</div>
<script type="math/tex; mode=display">
f_{i+1}-2 f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=2, \ldots, N-1
</script>
</div>
<p>At the lower boundary point <span><span class="MathJax_Preview">i = 1</span><script type="math/tex">i = 1</script></span> the neighbor <span><span class="MathJax_Preview">i − 1</span><script type="math/tex">i − 1</script></span> does not exist in our domain, and, similarly, at the upper boundary point <span><span class="MathJax_Preview">i = N</span><script type="math/tex">i = N</script></span> the point <span><span class="MathJax_Preview">i + 1</span><script type="math/tex">i + 1</script></span> does not exist. At these points we have to implement the boundary conditions, which can be done in many different ways.</p>
<p>Let us assume that the solution f is a symmetric function about <span><span class="MathJax_Preview">x = 0</span><script type="math/tex">x = 0</script></span>, in which case we can restrict the analysis to positive x and impose a Neuman condition at the origin,</p>
<div>
<div class="MathJax_Preview">
\partial_{x} f=0 \quad \text { at } x=0
</div>
<script type="math/tex; mode=display">
\partial_{x} f=0 \quad \text { at } x=0
</script>
</div>
<p>The two grid points <span><span class="MathJax_Preview">x_0</span><script type="math/tex">x_0</script></span> and <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span> then bracket the boundary point <span><span class="MathJax_Preview">x_{min} = 0</span><script type="math/tex">x_{min} = 0</script></span> symmetrically. We can then write the boundary condition as</p>
<div>
<div class="MathJax_Preview">
f_{1}=f_{0}
</div>
<script type="math/tex; mode=display">
f_{1}=f_{0}
</script>
</div>
<p>For i = 1 we yields</p>
<div>
<div class="MathJax_Preview">
f_{i+1}-f_{i}=(\Delta x)^{2} s_{i} \quad i=1
</div>
<script type="math/tex; mode=display">
f_{i+1}-f_{i}=(\Delta x)^{2} s_{i} \quad i=1
</script>
</div>
<p>We can use a similar strategy at the upper boundary. Let us also assume that f falls off with <span><span class="MathJax_Preview">1/x</span><script type="math/tex">1/x</script></span> for large x, which results in the Robin boundary condition</p>
<div>
<div class="MathJax_Preview">
\partial_{x}(x f)=0 \quad \text { as } x \rightarrow \infty
</div>
<script type="math/tex; mode=display">
\partial_{x}(x f)=0 \quad \text { as } x \rightarrow \infty
</script>
</div>
<p>With the help of a virtual grid point <span><span class="MathJax_Preview">x_{N + 1}</span><script type="math/tex">x_{N + 1}</script></span> we can write the boundary condition in <span><span class="MathJax_Preview">\Delta x</span><script type="math/tex">\Delta x</script></span> as</p>
<div>
<div class="MathJax_Preview">
f_{N+1}=\frac{x_{N}}{x_{N+1}} f_{N}=\frac{x_{N}}{x_{N}+\Delta x} f_{N}
</div>
<script type="math/tex; mode=display">
f_{N+1}=\frac{x_{N}}{x_{N+1}} f_{N}=\frac{x_{N}}{x_{N}+\Delta x} f_{N}
</script>
</div>
<p>We can again insert this into for i = N and find</p>
<div>
<div class="MathJax_Preview">
\left(\frac{x_{i}}{x_{i}+\Delta x}-2\right) f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=N
</div>
<script type="math/tex; mode=display">
\left(\frac{x_{i}}{x_{i}+\Delta x}-2\right) f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=N
</script>
</div>
<p>Elliptic Equations now form a coupled set of N linear equations for the N elements <span><span class="MathJax_Preview">f_i</span><script type="math/tex">f_i</script></span> that we can write as</p>
<div>
<div class="MathJax_Preview">
\left( \begin{array}{ccccccc}{-1} &amp; {1} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\ {1} &amp; {-2} &amp; {1} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\ {0} &amp; {\ddots} &amp; {\ddots} &amp; {\ddots} &amp; {0} &amp; {0} &amp; {0} \\ {0} &amp; {0} &amp; {1} &amp; {-2} &amp; {1} &amp; {0} &amp; {0} \\ {0} &amp; {0} &amp; {0} &amp; {\ddots} &amp; {\ddots} &amp; {\ddots} &amp; {0} \\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {1} &amp; {-2} &amp; {1} \\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {1} &amp; {x_{N} /\left(x_{N}+\Delta x\right)-2}\end{array}\right) \cdot \left( \begin{array}{c}{f_{1}} \\ {f_{2}} \\ {\vdots} \\ {f_{i}} \\ {\vdots} \\ {f_{N-1}} \\ {f_{N}}\end{array}\right)=(\Delta x)^{2} \left( \begin{array}{c}{s_{1}} \\ {s_{2}} \\ {\vdots} \\ {s_{i}} \\ {\vdots} \\ {s_{N-1}} \\ {s_{N}}\end{array}\right)
</div>
<script type="math/tex; mode=display">
\left( \begin{array}{ccccccc}{-1} & {1} & {0} & {0} & {0} & {0} & {0} \\ {1} & {-2} & {1} & {0} & {0} & {0} & {0} \\ {0} & {\ddots} & {\ddots} & {\ddots} & {0} & {0} & {0} \\ {0} & {0} & {1} & {-2} & {1} & {0} & {0} \\ {0} & {0} & {0} & {\ddots} & {\ddots} & {\ddots} & {0} \\ {0} & {0} & {0} & {0} & {1} & {-2} & {1} \\ {0} & {0} & {0} & {0} & {0} & {1} & {x_{N} /\left(x_{N}+\Delta x\right)-2}\end{array}\right) \cdot \left( \begin{array}{c}{f_{1}} \\ {f_{2}} \\ {\vdots} \\ {f_{i}} \\ {\vdots} \\ {f_{N-1}} \\ {f_{N}}\end{array}\right)=(\Delta x)^{2} \left( \begin{array}{c}{s_{1}} \\ {s_{2}} \\ {\vdots} \\ {s_{i}} \\ {\vdots} \\ {s_{N-1}} \\ {s_{N}}\end{array}\right)
</script>
</div>
<p>or, in a more compact form,</p>
<div>
<div class="MathJax_Preview">
\mathbf{A} \cdot \mathbf{f}=(\Delta x)^{2} \mathbf{S}
</div>
<script type="math/tex; mode=display">
\mathbf{A} \cdot \mathbf{f}=(\Delta x)^{2} \mathbf{S}
</script>
</div>
<p>The solution is given by</p>
<div>
<div class="MathJax_Preview">
\mathbf{f}=(\Delta x)^{2} \mathbf{A}^{-1} \cdot \mathbf{S}
</div>
<script type="math/tex; mode=display">
\mathbf{f}=(\Delta x)^{2} \mathbf{A}^{-1} \cdot \mathbf{S}
</script>
</div>
<p>so that we have reduced the problem to inverting an N × N matrix.</p>
<h4 id="hyperbolic-equations">Hyperbolic Equations</h4>
<p>For simplicity it does not contain any source terms, and the the wave speed v is constant.</p>
<div>
<div class="MathJax_Preview">
\partial _ { t } u + v \partial _ { x } u = 0
</div>
<script type="math/tex; mode=display">
\partial _ { t } u + v \partial _ { x } u = 0
</script>
</div>
<p>The equation is satisfied exactly by any function of the form <span><span class="MathJax_Preview">u ( t , x ) = u ( x - v t )</span><script type="math/tex">u ( t , x ) = u ( x - v t )</script></span>. <strong>The equation has a time derivative in addition to the space derivative, and thus requires initial data</strong>.</p>
<p>Inserting both finite-difference representations</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } u \right) _ { j } ^ { n } = \frac { u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right) \\
\left( \partial _ { t } u \right) _ { j } ^ { n } = \frac { u _ { j } ^ { n + 1 } - u _ { j } ^ { n } } { \Delta t } + \mathcal { O } ( \Delta t )
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } u \right) _ { j } ^ { n } = \frac { u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right) \\
\left( \partial _ { t } u \right) _ { j } ^ { n } = \frac { u _ { j } ^ { n + 1 } - u _ { j } ^ { n } } { \Delta t } + \mathcal { O } ( \Delta t )
</script>
</div>
<p>we can solve for <span><span class="MathJax_Preview">u^{n+1}_j</span><script type="math/tex">u^{n+1}_j</script></span> and find</p>
<div>
<div class="MathJax_Preview">
u _ { j } ^ { n + 1 } = u _ { j } ^ { n } - \frac { v } { 2 } \frac { \Delta t } { \Delta x } \left( u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } \right)
</div>
<script type="math/tex; mode=display">
u _ { j } ^ { n + 1 } = u _ { j } ^ { n } - \frac { v } { 2 } \frac { \Delta t } { \Delta x } \left( u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } \right)
</script>
</div>
<p>or reasons that are quite obivous this differencing scheme is called forward-time centered-space, or FTCS.</p>
<p><img alt="" src="../media/15512306516707.jpg" /></p>
<p>It is an example of an explicit scheme, meaning that we can solve for the grid function <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> at the new time level n + 1 directly in terms of function values on the old time level n.</p>
<h5 id="courant-friedrichs-lewy-condition">Courant-Friedrichs-Lewy condition</h5>
<p>Unfortunately, however, FTCS is fairly useless. The equation is satisfied exactly by any function of the form <span><span class="MathJax_Preview">u ( t , x ) = u ( x - v t )</span><script type="math/tex">u ( t , x ) = u ( x - v t )</script></span>. we can write the solution <span><span class="MathJax_Preview">u ( t , x )</span><script type="math/tex">u ( t , x )</script></span> to our continuum hyperbolic differential equation as a superposition of eigenmodes <span><span class="MathJax_Preview">e^{i(\omega t+k x)}</span><script type="math/tex">e^{i(\omega t+k x)}</script></span>. Here k is a spatial wave number.</p>
<p>A real <span><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span>, for which <span><span class="MathJax_Preview">e^{i \omega t}</span><script type="math/tex">e^{i \omega t}</script></span> has a magnitude of unity, yields sinusoidally oscillating modes, while the existence of a complex piece in <span><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> leads to exponentially growing or damping modes. In the case of exponential growth, the magnitude of <span><span class="MathJax_Preview">e^{i \omega t}</span><script type="math/tex">e^{i \omega t}</script></span> will exceed unity.</p>
<p>We can perform a similar spectral analysis of the finite difference equation. Write the eigenmode for <span><span class="MathJax_Preview">u_{j}^{n}</span><script type="math/tex">u_{j}^{n}</script></span> as</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n}=\xi^{n} e^{i k(j \Delta x)}
</div>
<script type="math/tex; mode=display">
u_{j}^{n}=\xi^{n} e^{i k(j \Delta x)}
</script>
</div>
<p>Here the quantity <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> plays the role of <span><span class="MathJax_Preview">e^{i \omega \Delta t}</span><script type="math/tex">e^{i \omega \Delta t}</script></span> and is called the amplification factor:</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n}=\xi u_{j}^{n-1} = \xi^{2} u_{j}^{n-2} \ldots=\xi^{n} u_{j}^{0}
</div>
<script type="math/tex; mode=display">
u_{j}^{n}=\xi u_{j}^{n-1} = \xi^{2} u_{j}^{n-2} \ldots=\xi^{n} u_{j}^{0}
</script>
</div>
<p>For the scheme to be stable, the magnitude <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> must be smaller or equal to unity for all k,</p>
<div>
<div class="MathJax_Preview">
|\xi(k)| \leq 1
</div>
<script type="math/tex; mode=display">
|\xi(k)| \leq 1
</script>
</div>
<p>To perform a von Neumann stability anaylsis of the FTCS scheme</p>
<div>
<div class="MathJax_Preview">
\xi(k)=1-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</div>
<script type="math/tex; mode=display">
\xi(k)=1-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</script>
</div>
<p><strong>the magnitude of <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> is greater than unity for all k, indicating that this scheme is unstable.</strong> In fact, we have <span><span class="MathJax_Preview">|\xi|&gt;1</span><script type="math/tex">|\xi|>1</script></span> independently of our choice for <span><span class="MathJax_Preview">\Delta x</span><script type="math/tex">\Delta x</script></span> and <span><span class="MathJax_Preview">\Delta t</span><script type="math/tex">\Delta t</script></span>, which makes this scheme unconditionally unstable. That is bad. <strong>The good news is that there are several ways of fixing this problem.</strong></p>
<p>For example, we could replace the term <span><span class="MathJax_Preview">u_{j}^{n}</span><script type="math/tex">u_{j}^{n}</script></span> by the spatial average <span><span class="MathJax_Preview">\left(u_{j+1}^{n}+u_{j-1}^{n}\right) / 2</span><script type="math/tex">\left(u_{j+1}^{n}+u_{j-1}^{n}\right) / 2</script></span>.</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=\frac{1}{2}\left(u_{j+1}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=\frac{1}{2}\left(u_{j+1}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>a von Neumann analysis results in the amplification factor</p>
<div>
<div class="MathJax_Preview">
\xi=\cos k \Delta x-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</div>
<script type="math/tex; mode=display">
\xi=\cos k \Delta x-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</script>
</div>
<p>The von Neumann stability criterion then implies that we must have</p>
<div>
<div class="MathJax_Preview">
\frac { | v | \Delta t } { \Delta x } \leq 1
</div>
<script type="math/tex; mode=display">
\frac { | v | \Delta t } { \Delta x } \leq 1
</script>
</div>
<p>The Courant condition states that the the grid point <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> at the new time level n+1 has to reside inside the domain of determinacy of the interval spanned by the finite difference stencil at the time level n. This makes intuitive sense: if <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> were outside this domain, its physical specification would require more information about the past than we are providing numerically, which may trigger an instability.</p>
<p><img alt="" src="../media/15512306641761.jpg" /></p>
<p>Recalling that v represents the speed of a characteristic, we may interpret the Courant condition in terms of the domain of determinacy.</p>
<p><strong>It seems somewhat like a miracle that simply replacing a grid function by a local average manages to change the numerical scheme from unconditionally unstable to conditionally stable.</strong></p>
<p>This change can be interpreted in very physical terms</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n}+\frac{1}{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n}+\frac{1}{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>or</p>
<div>
<div class="MathJax_Preview">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \frac{u_{j+1}^{n}-u_{j-1}^{n}}{2 \Delta x}+\frac{(\Delta x)^{2}}{2 \Delta t} \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{(\Delta x)^{2}}
</div>
<script type="math/tex; mode=display">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \frac{u_{j+1}^{n}-u_{j-1}^{n}}{2 \Delta x}+\frac{(\Delta x)^{2}}{2 \Delta t} \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{(\Delta x)^{2}}
</script>
</div>
<p>But equation is a finite-difference representation of the differential equation</p>
<div>
<div class="MathJax_Preview">
\partial_{t} u+v \partial_{x} u=D \partial_{x}^{2} u
</div>
<script type="math/tex; mode=display">
\partial_{t} u+v \partial_{x} u=D \partial_{x}^{2} u
</script>
</div>
<p>where the term on the right-hand side is essentially a diffusion term, with parameter <span><span class="MathJax_Preview">D=(\Delta x)^{2} /(2 \Delta t)</span><script type="math/tex">D=(\Delta x)^{2} /(2 \Delta t)</script></span> serving as a constant coefficient of diffusion.</p>
<p><strong>This feature implies the amplitude of any wave will decrease spuriously with time as it propagates. A related effect is anomalous dispersion, an additional price we pay for stablity in the Lax scheme and many other finite-difference schemes for hyperbolic systems.</strong></p>
<h5 id="implicit-scheme">implicit scheme</h5>
<!--Lax scheme

___

There are a number of other ways of constructing stable finite difference schemes for the model equation. A popular alternative to the Lax scheme is upwind differencing

$$
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \left\{\begin{array}{l}{\frac{u_{j}^{n}-u_{j-1}^{n}}{\Delta x},} & {v>0} \\ {\frac{u_{j+1}^{n}-u_{j}^{n}}{\Delta x},} & {v>0}\end{array}\right.
$$

This scheme borrows its name from the fact that for a wave with $v>0$, that travels “to the right”, say, the new grid-point $u_{j}^{n+1}$ is affected only by the “upwind” grid-points to its left, i.e. that lie in the region through which the wave travels before reaching $x_{j}$.

three-level scheme

___

It would be desirable, however, to have a scheme that is second order in both in space and time. One way to construct such a code is to abandon two-level schemes, and instead consider a three-level scheme. We can then construct centered derivatives both for the time-derivative,

$$
\left(\partial_{t} u\right)_{j}^{n}=\frac{u_{j}^{n+1}-u_{j}^{n-1}}{2 \Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
$$

the leap-frog scheme,

$$
u_{j}^{n+1}=u_{j}^{n-1}-v \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
$$

Some researchers prefer two-level schemes over three-level schemes because three level schemes require initial data on two different time levels, which can be somewhat awkward.

![-w619](media/15532637365521.jpg)

The leap-frog scheme has the additional disadvantage that, it only connects fields of the same color. “Black” gridpoints can therefore evolve completely independently of “white” gridpoints, and the two sets of grid points may drift apart as numerical error accumulates differently for the two sets of points. If necessary, this problem can be solved by artificially adding a very small viscous term that links the two sets together. Once these potential issues are resolved, leap-frog is a very simple, accurate and powerful method.
-->

<p>Yet another way of constructing a stable two-level scheme is to use backward time differencing instead of forward differencing. This approach then yields the “backward-time, centered-space” scheme,</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)
</script>
</div>
<p><img alt="-w600" src="../media/15532642037929.jpg" /></p>
<p>Performing a von Neumann stability analysis we find the amplification factor</p>
<div>
<div class="MathJax_Preview">
|\xi(k)| \leq 1
</div>
<script type="math/tex; mode=display">
|\xi(k)| \leq 1
</script>
</div>
<p>for all values of <span><span class="MathJax_Preview">\Delta t</span><script type="math/tex">\Delta t</script></span>. This finding means that this scheme is unconditionally stable. <strong>The size of the stepsize ∆t is no longer restricted by stability, and instead is limited only by accuracy requirements.</strong> this property is even more important for parabolic equations.</p>
<p>The disadvantage of the backward differencing scheme is that we can no longer solve for the new grid function <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> at the new time <span><span class="MathJax_Preview">t^{n+1}</span><script type="math/tex">t^{n+1}</script></span> explicitly in terms of old grid functions at <span><span class="MathJax_Preview">t^{n}</span><script type="math/tex">t^{n}</script></span> alone. Instead, now couples <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> with the its closest neighbors <span><span class="MathJax_Preview">u_{j+1}^{n+1}</span><script type="math/tex">u_{j+1}^{n+1}</script></span> and <span><span class="MathJax_Preview">u_{j-1}^{n+1}</span><script type="math/tex">u_{j-1}^{n+1}</script></span>. This coupling provides an implicit linear relation between the new grid functions, and is therefore an example of an implicit finite-differencing scheme. We can no longer sweep through the grid and update one point at a time; instead <strong>we now have to solve for all grid points simultaneously</strong>. Writing equation at all interior grid points, and then taking into account the boundary conditions, leads to a system of equations quite similar to elliptic equations.</p>
<!--Crank-Nicholson scheme

___

A second-order scheme would be time-centered, meaning that we should estimate the the time derivative at the mid-point between the two time levels n and n + 1,

Implementing time-centering means that we also have to evaluate the space derivative at this same midpoint $n+1/2$  which we can do by averaging between the values at n and n+1.

$$
\left(\partial_{t} u\right)_{j}^{n+1 / 2}=\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
$$

This approach yields the Crank-Nicholson scheme

$$
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{4} \frac{\Delta t}{\Delta x}\left(\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right)\right)
$$

![-w696](media/15532650020363.jpg)

Crank-Nicholson is second order in both space and time, and shows that it is unconditionally stable.

The iterative Crank-Nicholson scheme uses a predictor-corrector approach. In the predictor step we predict the new values $u_{j}^{n+1}$ by using the fully explicit FTCS scheme.

$$
^{(1)} u_{j}^{n+1}=u_{j}^{n}-\frac{v \Delta t}{2 \Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
$$

which, as we have seen above, would be unconditionally unstable by itself. In a subsequent corrector step we use these predicted values $^{(1)}{u_{j}}^{n+1}$ together with the $u_{j}^{n}$ to obtain a time-centered approximation for the spatial derivative on the right-hand side of equation. This step yields the corrected values of the grid function,

$$
^{(2)} u_{j}^{n+1}=u_{j}^{n} - \frac{v \Delta t}{4 \Delta x} \left(\left( ^{(1)} u_{j+1}^{n+1} - ^{(1)} u_{j-1}^{n+1}\right)\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right) )
$$

The corrector step can be repeated an arbitrary number times N, always using the previous values $^{(N-1)} u_{j}^{n+1}$ on the right-hand side to find new corrected values $^{(N)} u_{j}^{n+1}$.

The iterative Crank-Nicholson scheme is an explicit two-level scheme that is second order in both space and time. Since this form is very similar to the 3 + 1 equations and related formulations, the iterative Crank-Nicholson scheme has often been used in numerical relativity simulations.
-->

<h5 id="method-of-lines">Method of Lines</h5>
<p>A popular alternative to these complete finite difference schemes is therefore the method of lines (or MOL for short).</p>
<p><strong>The basic idea of the method of lines is to finite difference the space derivatives only.</strong> Now we introduce a spatial grid only, at least for now, so that the function values at these gridpoint, <span><span class="MathJax_Preview">u_{j}(t)=u\left(t, x_{j}\right)</span><script type="math/tex">u_{j}(t)=u\left(t, x_{j}\right)</script></span>, remain functions of time. As a result, our partial differential equation for <span><span class="MathJax_Preview">u(t, x)</span><script type="math/tex">u(t, x)</script></span> becomes a set of ordinary differential equations for the grid values <span><span class="MathJax_Preview">u_{j}(t)</span><script type="math/tex">u_{j}(t)</script></span>. The next question is how to integrate the ordinary differential equations. The appealing feature of the method of lines, however, is that we can use any method for the integration of the ordinary differential equations that we like. In fact, many such methods, including very efficient, high-order methods, are precoded and readily available. One such algorithm is the ever-popular Runge-Kutta method.</p>
<p>To implement, say, a fourth-order scheme for our model</p>
<div>
<div class="MathJax_Preview">
\partial_{t} u+v \partial_{x} u=0
</div>
<script type="math/tex; mode=display">
\partial_{t} u+v \partial_{x} u=0
</script>
</div>
<p>we could adopt the fourth-order differencing stencil to replace the spatial derivative, yielding</p>
<div>
<div class="MathJax_Preview">
\frac{d u_{j}}{d t}=-\frac{v}{12 \Delta x}\left(u_{i-2}-8 u_{j-1}+8 u_{j+1}-u_{i+2}\right)
</div>
<script type="math/tex; mode=display">
\frac{d u_{j}}{d t}=-\frac{v}{12 \Delta x}\left(u_{i-2}-8 u_{j-1}+8 u_{j+1}-u_{i+2}\right)
</script>
</div>
<p>and then integrate this set of ordinary differential equations with a fourth-order Runge-Kutta method.</p>
<h6 id="runge-kutta-rk-method">Runge-Kutta (RK) method</h6>
<p>The explicit form of the algorithms is</p>
<ol>
<li>Prediction step (common for both RK2 and RK3):</li>
</ol>
<div>
<div class="MathJax_Preview">
\boldsymbol{U}^{(1)}=\boldsymbol{U}^{(n)}+\Delta t L\left(\boldsymbol{U}^{(n)}\right)
</div>
<script type="math/tex; mode=display">
\boldsymbol{U}^{(1)}=\boldsymbol{U}^{(n)}+\Delta t L\left(\boldsymbol{U}^{(n)}\right)
</script>
</div>
<ol>
<li>
<p>Depending on the order do:</p>
</li>
<li>
<p>RK2:</p>
</li>
</ol>
<div>
<div class="MathJax_Preview">
\boldsymbol{U}^{n+1}=\frac{1}{\alpha}\left[\beta \boldsymbol{U}^{n}+\boldsymbol{U}^{(1)}+\Delta t L\left(\boldsymbol{U}^{(1)}\right)\right]
</div>
<script type="math/tex; mode=display">
\boldsymbol{U}^{n+1}=\frac{1}{\alpha}\left[\beta \boldsymbol{U}^{n}+\boldsymbol{U}^{(1)}+\Delta t L\left(\boldsymbol{U}^{(1)}\right)\right]
</script>
</div>
<ul>
<li>RK3:</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} \boldsymbol{U}^{(2)} &amp;=\frac{1}{\alpha}\left[\beta \boldsymbol{U}^{(2 n)}+\boldsymbol{U}^{(1)}+\Delta t L\left(\boldsymbol{U}^{(1)}\right)\right] \\ \boldsymbol{U}^{n+1} &amp;=\frac{1}{\beta}\left[\beta \boldsymbol{U}^{(2 n)}+2 \boldsymbol{U}^{(2)}+2 \Delta t L\left(\boldsymbol{U}^{(2)}\right)\right] \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{U}^{(2)} &=\frac{1}{\alpha}\left[\beta \boldsymbol{U}^{(2 n)}+\boldsymbol{U}^{(1)}+\Delta t L\left(\boldsymbol{U}^{(1)}\right)\right] \\ \boldsymbol{U}^{n+1} &=\frac{1}{\beta}\left[\beta \boldsymbol{U}^{(2 n)}+2 \boldsymbol{U}^{(2)}+2 \Delta t L\left(\boldsymbol{U}^{(2)}\right)\right] \end{aligned}
</script>
</div>
<h3 id="ghost-zones">Ghost Zones</h3>
<p>Cactus is based upon a distributed computing paradigm. That is, the problem domain is split into blocks, each of which is assigned to a processor.</p>
<p>Consider the 1-D wave equation</p>
<div>
<div class="MathJax_Preview">
\frac{\partial^{2} \phi}{\partial t^{2}}=\frac{\partial^{2} \phi}{\partial x^{2}}
</div>
<script type="math/tex; mode=display">
\frac{\partial^{2} \phi}{\partial t^{2}}=\frac{\partial^{2} \phi}{\partial x^{2}}
</script>
</div>
<p>To solve this by partial differences, one discretises the derivatives to get an equation relating the solution
at different times.</p>
<div>
<div class="MathJax_Preview">
\phi(t+\Delta t, x)-2 \phi(t, x)+\phi(t-\Delta t, x)=\frac{\Delta t^{2}}{\Delta x^{2}}\{\phi(t, x+\Delta x)-2 \phi(t, x)+\phi(t, x-\Delta x)\}
</div>
<script type="math/tex; mode=display">
\phi(t+\Delta t, x)-2 \phi(t, x)+\phi(t-\Delta t, x)=\frac{\Delta t^{2}}{\Delta x^{2}}\{\phi(t, x+\Delta x)-2 \phi(t, x)+\phi(t, x-\Delta x)\}
</script>
</div>
<p>On examination, you can see that to generate the data at the point <span><span class="MathJax_Preview">(t+\Delta t, x)</span><script type="math/tex">(t+\Delta t, x)</script></span> we need data from the four points <span><span class="MathJax_Preview">(t, x)</span><script type="math/tex">(t, x)</script></span>, <span><span class="MathJax_Preview">(t -\Delta t, x)</span><script type="math/tex">(t -\Delta t, x)</script></span>, <span><span class="MathJax_Preview">(t, x + \Delta x)</span><script type="math/tex">(t, x + \Delta x)</script></span> and <span><span class="MathJax_Preview">(t, x- \Delta x)</span><script type="math/tex">(t, x- \Delta x)</script></span> only.</p>
<p>Now, if you evolve the above scheme, it becomes apparent that at each iteration the number of grid points you can evolve decreases by one at each edge.</p>
<p><img alt="-w816" src="../media/15528964483464.jpg" /></p>
<p>At the outer boundary of the physical domain, the data for the boundary point can be generated by the boundary conditions, however, at internal boundaries, the data has to be copied from the adjacent processor.</p>
<p>It would be inefficient to copy each point individually, so instead, a number of ghostzones are created at the internal boundaries. A ghostzone consists of a copy of the whole plane (in 3D, line in 2D, point in 1D) of the data from the adjacent processor.</p>
<p><img alt="-w785" src="../media/15528964642193.jpg" /></p>
<p>Once the data has been evolved one step, the data in the ghostzones can be exchanged (or synchronised) between processors in one fell swoop before the next evolution step.</p>
<h3 id="mesh-refinement">Mesh Refinement</h3>
<p>It is often the case in simulations of physical systems that <strong>the most interesting phenomena may occur in only a subset of the computational domain.</strong> In the other regions of the domain it may be possible to use a less accurate approximation, thereby reducing the computational resources required, and still obtain results which are essentially similar to those obtained if no such reduction is made.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Imagine, for concreteness, a simulation of a strong-field gravitational wave source, like a compact binary containing neutron stars or black holes. On the one hand we have to resolve these sources well, so as to minimize truncation error in the strong-field region. On the other hand, the grid must extend into the weak-field region at large distances from the sources, so as to minimize error from the outer boundaries and to enable us to extract the emitted gravitational radiation accurately.</p>
</div>
<p><img alt="-w1062" src="../media/15533057187758.jpg" /></p>
<p>In particular, we may consider using a computational mesh which is non-uniform in space and time, using <strong>a finer mesh resolution in the “interesting” regions</strong> where we expect it to be necessary, and using <strong>a coarser resolution in other areas</strong>. <mark>This is what we mean by mesh refinement (MR).</mark></p>
<p>The mesh refinement driver that we use is called Carpet and is available together with the application framework Cactus. It uses the Berger–Oliger approach, where the computational domain as well as all refined subdomains consist of a set of rectangular grids. <strong>Furthermore, there is a constant refinement ratio between refinement levels.</strong> The basic idea underlying mesh refinement techniques is to perform the simulation not on one numerical grid, but on several, as in the multigrid methods.</p>
<p><img alt="" src="../media/15523870938244.jpg" /></p>
<p><img alt="" src="../media/15551363353554.jpg" /></p>
<div class="admonition note">
<p class="admonition-title">notation</p>
<p>The grids are grouped into refinement levels (or simply “levels”) <span><span class="MathJax_Preview">L^K</span><script type="math/tex">L^K</script></span>, each containing an arbitrary number of grids <span><span class="MathJax_Preview">G^{k}_{j}</span><script type="math/tex">G^{k}_{j}</script></span>. Each grid on refinement level k has the grid spacing (in one dimension) <span><span class="MathJax_Preview">\Delta x^{k}</span><script type="math/tex">\Delta x^{k}</script></span>. The grid spacings are related by the relation <span><span class="MathJax_Preview">\Delta x^{k}=\Delta x^{k-1} / N_{\text { refine }}</span><script type="math/tex">\Delta x^{k}=\Delta x^{k-1} / N_{\text { refine }}</script></span> with the integer refinement factor <span><span class="MathJax_Preview">N_{\text { refine }}</span><script type="math/tex">N_{\text { refine }}</script></span>. The base level <span><span class="MathJax_Preview">L^0</span><script type="math/tex">L^0</script></span> covers the entire domain (typically with a single grid) using a coarse grid spacing. The refined grids have to be properly nested. That is, any grid <span><span class="MathJax_Preview">G^{k}_{j}</span><script type="math/tex">G^{k}_{j}</script></span> must be completely contained within the set of grids <span><span class="MathJax_Preview">L^{k-1}</span><script type="math/tex">L^{k-1}</script></span> of the next coarser level, except possibly at the outer boundaries.</p>
</div>
<p>The times and places where refined grids are created and removed are decided by some refinement criterion. The simplest criterion, which is also indispensable for testing, is <strong>manually specifying the locations of the refined grids at fixed locations in space at all times. This is called fixed mesh refinement.</strong> A bit more involved is keeping the same refinement hierarchy, but <strong>moving the finer grids according to some knowledge about the simulated system</strong>, tracking some feature such as a black hole or a neutron star. This might be called “moving fixed mesh refinement”. Clearly the most desirable strategy is an automatic criterion that estimates the truncation error, and places the refined grids only when and where necessary. This is what is commonly understood by adaptive mesh refinement. Carpet supports all of the above in principle.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The situation is more complicated for objects that are moving, as is the case for a coalescing binary star system. In this case we do not know a priori the trajectories of the companion stars, hence do not know which regions need refining. Moreover, these regions will be changing as the system evolves and the stars move. Clearly, we would like to move the refined grids with the stars. Such an approach, whereby the grid is relocated during the simulation to give optimal resolution at each time step, is called adaptive mesh refinement or AMR.</p>
</div>
<h4 id="time-evolution-scheme">Time evolution scheme</h4>
<p>In multigrid methods, the numerical solution is computed on a hierarchy of computational grids with increasing grid resolution. The finer grids may or may not cover all the physical space that is covered by the coarser grids. The numerical solution is then computed by completing sweeps through the grid hierarchy. </p>
<p><img alt="-w731" src="../media/15533068726818.jpg" /></p>
<h5 id="prolongation">Prolongation</h5>
<p>The coarse grid is sufficiently small so that we can compute a solution with a direct solver. This provides the “global” features of the solution, albeit on a coarse grid and hence with a large local truncation error. We then interpolate this approximate solution to the next finer grid. This interpolation from a coarser grid to a finer grid is called a “prolongation”.</p>
<blockquote>
<p>In the mathematical field of numerical analysis, interpolation is a method of constructing new data points within the range of a discrete set of known data points.</p>
</blockquote>
<p>AMR scheme evolves coarse grid data forward in time before evolving any data on the finer grids. These evolved coarse grid data can then be used to <strong>provide boundary conditions for the evolution of data on the finer grids via prolongation</strong>, i.e. interpolation in time and space.</p>
<p><img alt="" src="../media/15551377083235.jpg" /></p>
<p>A refinement by a factor of <span><span class="MathJax_Preview">N_{\text { refine }}</span><script type="math/tex">N_{\text { refine }}</script></span> requires time step sizes that are smaller by a factor <span><span class="MathJax_Preview">N_{\text { refine }}</span><script type="math/tex">N_{\text { refine }}</script></span>, and hence <span><span class="MathJax_Preview">N_{\text { refine }}</span><script type="math/tex">N_{\text { refine }}</script></span> time steps on level <span><span class="MathJax_Preview">k+1</span><script type="math/tex">k+1</script></span> are necessary for each time step on level k.</p>
<h5 id="restriction">Restriction</h5>
<p>At time steps in which the coarse and fine grids are both defined, the fine grid data are restricted onto the coarse grid (via a simple copy operation) after it has been evolved forward in time.</p>
<p><img alt="" src="../media/15551378176639.jpg" /></p>
<p>The coarser grids now “learn” from the finer grids by comparing their last solution with the one that comes back from a finer grid. This comparison provides an estimate for the local truncation error. These sweeps through the grid hierarchy can be repeated until the solution has converged to a pre-determined accuracy.</p>
<p>If there are more than two grid levels, then one proceeds recursively from coarsest to finest, evolving data on the coarsest grid first, interpolating this data in time and space along boundaries of finer grids, evolving the finer grid data, and restricting evolved data from finer to coarser grids whenever possible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For time evolution schemes that consist only of a single iteration (or step), the fine grid boundary condition needs to be applied only once. Most higher-order time integrations schemes, such as Runge-Kutta or iterative Crank-Nicholson, are actually multi-step schemes and correspondingly require <strong>the fine grid boundary condition to be applied multiple times</strong>. If this is not done in a consistent manner at each iteration, then the coarse and the fine grid time evolution will not couple correctly, and this can introduce a significant error.</p>
<p>There are several ways to guarantee consistent boundary conditions on fine grids. Our method use a larger fine grid boundary. That is, each of the integration substeps is formally applied to a progressively smaller domain, and the prolongation operation re-enlarges the domain back to its original size.</p>
<p><img alt="" src="../media/15551389503538.jpg" /></p>
<p>Note that this “buffering” is done only for prolongation boundaries; outer boundaries are handled in the conventional way. Note also that the use of buffer zones is potentially more computationally efficient. We emphasise that the use of these buffer zones is not always necessary. To our knowledge the buffer zones are necessary only when the system of equations contains second spatial derivatives, and a multi-step numerical method is used for time integration.</p>
</div>
<h4 id="inter-grid-transport-operators">Inter-grid transport operators</h4>
<p>As described above, the interaction between the individual refinement levels happens via prolongation and restriction. For prolongation, Carpet currently supports polynomial interpolation, up to quadratic interpolation in time, which requires keeping at least two previous time levels of data. It also supports up to quintic interpolation in space, which requires using at least three ghost zones. We usually use cubic interpolation in space, which requires only two ghost zones. For restricting, Carpet currently uses sampling (i.e., a simple copy operation). These transport operators are not conservative. Since our formulation of Einstein’s equation is not in a conservative form, any use of conservative inter-grid operations offers no benefit. However, the transport operators can easily be changed.</p>
<h4 id="initial-data-generation">Initial data generation</h4>
<p>Initial data generation and time evolution are controlled by the driver Carpet. Initial data are created recursively, starting on the coarsest level <span><span class="MathJax_Preview">L^{0}</span><script type="math/tex">L^{0}</script></span>. </p>
<p>In many cases, the initial data specification is only valid for a single time <span><span class="MathJax_Preview">t=0</span><script type="math/tex">t=0</script></span>. However, for the time interpolation necessary during prolongation, it may be necessary to have data on several time levels. One solution is to use only lower order interpolation during the first few time steps. We decided instead, we offer the option to evolve coarse grid data backwards in time in order to provide sufficient time levels for higher order interpolation in time at fine grid boundaries. </p>
<p>This initial data generation proceeds in two stages.</p>
<p>First the data are evolved both forwards and backwards in time one step, leading to the “hourglass” structure.</p>
<p><img alt="" src="../media/15551393149475.jpg" /></p>
<p>This evolution proceeds recursively from coarsest to finest, so that all data necessary for time interpolation are present. Note that this would not be the case if we evolved two steps backwards in time, as there would not be enough data for the time interpolation for the restriction operation between these two steps.</p>
<h3 id="pseudo-spectral-methods">Pseudo-spectral methods</h3>
<p>In spectral methods the evolved fields are expressed in terms of a finite sum of basis functions. An example of this would be to describe a field on a sphere in terms of an expansion in spherical harmonics. These methods have the advantage that if the fields are smooth then <strong>the error in truncating the expansion converges to zero exponentially with the number of basis functions used in the expansion.</strong></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../SP/Introduction/" class="btn btn-neutral float-right" title="Introduction">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Matter Sources/" class="btn btn-neutral" title="Matter Sources"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/YuLiumt/Einstein-Toolkit/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../Matter Sources/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../SP/Introduction/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
