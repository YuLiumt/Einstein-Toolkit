<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Yu Liu">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Numerical Methods - Yu Liu's Notebook</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Numerical Methods";
    var mkdocs_page_input_path = "NR/Numerical Methods.md";
    var mkdocs_page_url = "/Einstein-Toolkit/NR/Numerical Methods/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-36723568-3', 'https://yuliumt.github.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Yu Liu's Notebook</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Einstein Toolkit</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../ET/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Install/">Install</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Cactus/">Cactus</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Thorn/">Thorn</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Source File/">Source File</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/parameter/">Parameter</a>
                </li>
                <li class="">
                    
    <a class="" href="../../ET/Visualization/">Visualization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">General Relativity</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../GR/Differential geometry/">Differential geometry</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GR/Einstein Equation/">Einstein Equation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GR/Solutions/">Solutions</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gravitational Wave</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../GW/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GW/Linearized Waves/">Linearized Waves</a>
                </li>
                <li class="">
                    
    <a class="" href="../../GW/Extracting Gravitational Waveforms/">Extracting Gravitational Waveforms</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Numerical Relativity</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../3+1 Decomposition/">3+1 Decomposition</a>
                </li>
                <li class="">
                    
    <a class="" href="../ADM/">ADM</a>
                </li>
                <li class="">
                    
    <a class="" href="../BSSN/">BSSN</a>
                </li>
                <li class="">
                    
    <a class="" href="../Conformal Transformation/">Conformal Transformation</a>
                </li>
                <li class="">
                    
    <a class="" href="../Coordinates/">Coordinates</a>
                </li>
                <li class="">
                    
    <a class="" href="../Matter Sources/">Matter Sources</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Numerical Methods</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#finite-difference-methods">Finite Difference Methods</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#elliptic-equations">Elliptic Equations</a></li>
        
            <li><a class="toctree-l4" href="#hyperbolic-equations">Hyperbolic Equations</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#ghost-zones">Ghost Zones</a></li>
    

    <li class="toctree-l3"><a href="#mesh-refinement">Mesh Refinement</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#fixed-mesh-refinement">Fixed Mesh Refinement</a></li>
        
            <li><a class="toctree-l4" href="#adaptive-mesh-refinement-amr">Adaptive mesh refinement (AMR)</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Search Pipeline</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../SP/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../SP/matched filter/">Matched Filtering</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Example</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Example/Poisson/">Poisson</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Yu Liu's Notebook</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Numerical Relativity &raquo;</li>
        
      
    
    <li>Numerical Methods</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/YuLiumt/Einstein-Toolkit/edit/master/docs/NR/Numerical Methods.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h3 id="finite-difference-methods">Finite Difference Methods</h3>
<p><strong>In a finite difference approximation a function <span><span class="MathJax_Preview">f(t,x)</span><script type="math/tex">f(t,x)</script></span> is represented by values at a discrete set of points.</strong> At the core of finite difference approximation is therefore a discretization of the spacetime, or a numerical grid. Instead of evaluating f at all values of x, for example, we only consider discrete values <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>. The distance between the gridpoints <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> is called the gridspacing <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>. For uniform grids, for which <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> is constant, we have</p>
<div>
<div class="MathJax_Preview">
x _ { i } = x _ { 0 } + i \Delta x
</div>
<script type="math/tex; mode=display">
x _ { i } = x _ { 0 } + i \Delta x
</script>
</div>
<p>If the solution depends on time we also discretize the time coordinate, for example as</p>
<div>
<div class="MathJax_Preview">
t ^ { n } = t ^ { 0 } + n \Delta t
</div>
<script type="math/tex; mode=display">
t ^ { n } = t ^ { 0 } + n \Delta t
</script>
</div>
<p>The finite difference representation of the function <span><span class="MathJax_Preview">f(t,x)</span><script type="math/tex">f(t,x)</script></span>, for example, is</p>
<div>
<div class="MathJax_Preview">
f _ { i } ^ { n } = f \left( t ^ { n } , x _ { i } \right) + \text { truncation error. }
</div>
<script type="math/tex; mode=display">
f _ { i } ^ { n } = f \left( t ^ { n } , x _ { i } \right) + \text { truncation error. }
</script>
</div>
<p>Differential equations involve derivatives, so we must next discuss how to represent derivatives in a finite difference representation.</p>
<p>Assuming that <span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> can be differentiated to sufficiently high order and that it can be represented as a Taylor series, we have</p>
<div>
<div class="MathJax_Preview">
f _ { i + 1 } = f \left( x _ { i } + \Delta x \right) = f \left( x _ { i } \right) + \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</div>
<script type="math/tex; mode=display">
f _ { i + 1 } = f \left( x _ { i } + \Delta x \right) = f \left( x _ { i } \right) + \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</script>
</div>
<p>Solving for <span><span class="MathJax_Preview">\left( \partial _ { x } f \right) _ { x _ { i } } = \left( \partial _ { x } f \right) _ { i }</span><script type="math/tex">\left( \partial _ { x } f \right) _ { x _ { i } } = \left( \partial _ { x } f \right) _ { i }</script></span> we find</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i } } { \Delta x } + \mathcal { O } ( \Delta x )
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i } } { \Delta x } + \mathcal { O } ( \Delta x )
</script>
</div>
<p>The truncation error of this expression is linear in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>, and it turns out that we can do better. We call equation a <strong>one-sided derivative</strong>, since it uses only neighbors on one side of <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>.</p>
<p>Consider the Taylor expansion to the point <span><span class="MathJax_Preview">x_{ i − 1 }</span><script type="math/tex">x_{ i − 1 }</script></span>,</p>
<div>
<div class="MathJax_Preview">
f _ { i - 1 } = f \left( x _ { i } - \Delta x \right) = f \left( x _ { i } \right) - \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</div>
<script type="math/tex; mode=display">
f _ { i - 1 } = f \left( x _ { i } - \Delta x \right) = f \left( x _ { i } \right) - \Delta x \left( \partial _ { x } f \right) _ { x _ { i } } + \frac { ( \Delta x ) ^ { 2 } } { 2 } \left( \partial _ { x } ^ { 2 } f \right) _ { x _ { i } } + \mathcal { O } \left( \Delta x ^ { 3 } \right)
</script>
</div>
<p>we now find</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i - 1 } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { f _ { i + 1 } - f _ { i - 1 } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</script>
</div>
<p>which is second order in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span>. In general, <strong>centered derivatives</strong> lead to higher order schemes than one-sided derivatives for the same number of gridpoints.</p>
<p>The key point is that we are able to combine the two Taylor expansions in such a way that the leading order error term cancels out, leaving us <strong>with a higher order representation of the derivative</strong>. This cancellation <strong>only works out for uniform grids</strong>, when <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> is independent of x. This is one of the reasons why many current numerical relativity applications of finite difference schemes work with uniform grids.</p>
<p>Higher order derivatives can be constructed in a similar fashion. Adding the two Taylor expansions all terms odd in <span><span class="MathJax_Preview">∆x</span><script type="math/tex">∆x</script></span> drop out and we find for the second derivative</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { f _ { i + 1 } - 2 f _ { i } + f _ { i - 1 } } { ( \Delta x ) ^ { 2 } } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { f _ { i + 1 } - 2 f _ { i } + f _ { i - 1 } } { ( \Delta x ) ^ { 2 } } + \mathcal { O } \left( \Delta x ^ { 2 } \right)
</script>
</div>
<blockquote>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } f \right) _ { i } = \frac { 1 } { 12 \Delta x } \left( f _ { i - 2 } - 8 f _ { i - 1 } + 8 f _ { i + 1 } - f _ { i + 2 } \right) \\
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { 1 } { 12 ( \Delta x ) ^ { 2 } } \left( - f _ { i - 2 } + 16 f _ { i - 1 } - 30 f _ { i } + 16 f _ { i + 1 } - f _ { i + 2 } \right)
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } f \right) _ { i } = \frac { 1 } { 12 \Delta x } \left( f _ { i - 2 } - 8 f _ { i - 1 } + 8 f _ { i + 1 } - f _ { i + 2 } \right) \\
\left( \partial _ { x } ^ { 2 } f \right) _ { i } = \frac { 1 } { 12 ( \Delta x ) ^ { 2 } } \left( - f _ { i - 2 } + 16 f _ { i - 1 } - 30 f _ { i } + 16 f _ { i + 1 } - f _ { i + 2 } \right)
</script>
</div>
<p>where we have omitted the truncation error, <span><span class="MathJax_Preview">\mathcal { O } \left( \Delta x ^ { 4 } \right)</span><script type="math/tex">\mathcal { O } \left( \Delta x ^ { 4 } \right)</script></span></p>
</blockquote>
<h4 id="elliptic-equations">Elliptic Equations</h4>
<p>As an example of a simple, one-dimensional elliptic equation consider</p>
<div>
<div class="MathJax_Preview">
\partial_{x}^{2} f=s
</div>
<script type="math/tex; mode=display">
\partial_{x}^{2} f=s
</script>
</div>
<p>We first have to construct a numerical grid that covers an interval between <span><span class="MathJax_Preview">x_{min}</span><script type="math/tex">x_{min}</script></span> and <span><span class="MathJax_Preview">x_{max}</span><script type="math/tex">x_{max}</script></span>. We then divide the interval <span><span class="MathJax_Preview">\left[x_{\min }, x_{\max }\right]</span><script type="math/tex">\left[x_{\min }, x_{\max }\right]</script></span> into N gridcells, leading to a gridspacing of</p>
<div>
<div class="MathJax_Preview">
\Delta x=\frac{x_{\max }-x_{\min }}{N}
</div>
<script type="math/tex; mode=display">
\Delta x=\frac{x_{\max }-x_{\min }}{N}
</script>
</div>
<p>We can choose our grid points to be located either at the center of these cells, which would be referred to as a cell-centered grid, or on the vertices, which would be referred to as a vertex-centered grid. For a cell-centered grid we have N grid points located at</p>
<div>
<div class="MathJax_Preview">
x_{i}=x_{\min }+(i-1 / 2) \Delta x, \quad i=1, \ldots, N
</div>
<script type="math/tex; mode=display">
x_{i}=x_{\min }+(i-1 / 2) \Delta x, \quad i=1, \ldots, N
</script>
</div>
<p><img alt="" src="../media/15516083713832.jpg" /></p>
<p>whereas for a vertex centered grid we have N + 1 gridpoints located a</p>
<div>
<div class="MathJax_Preview">
x_{i}=x_{\min }+(i-1) \Delta x, \quad i=1, \ldots, N+1
</div>
<script type="math/tex; mode=display">
x_{i}=x_{\min }+(i-1) \Delta x, \quad i=1, \ldots, N+1
</script>
</div>
<p><strong>The difference between cell-centered and vertex-centered grids only affects the implementation of boundary conditions, but not the finite difference representation of the differential equation itself.</strong></p>
<p>We are now ready to finite difference the differential equation. We define two arrays, <span><span class="MathJax_Preview">f_i</span><script type="math/tex">f_i</script></span> and <span><span class="MathJax_Preview">s_i</span><script type="math/tex">s_i</script></span>, which represent the functions f and s at the gridpoints <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> for <span><span class="MathJax_Preview">i = 1, . . . , N</span><script type="math/tex">i = 1, . . . , N</script></span>. In the interior of our domain we can represent the differential equation as</p>
<div>
<div class="MathJax_Preview">
f_{i+1}-2 f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=2, \ldots, N-1
</div>
<script type="math/tex; mode=display">
f_{i+1}-2 f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=2, \ldots, N-1
</script>
</div>
<p>At the lower boundary point <span><span class="MathJax_Preview">i = 1</span><script type="math/tex">i = 1</script></span> the neighbor <span><span class="MathJax_Preview">i − 1</span><script type="math/tex">i − 1</script></span> does not exist in our domain, and, similarly, at the upper boundary point <span><span class="MathJax_Preview">i = N</span><script type="math/tex">i = N</script></span> the point <span><span class="MathJax_Preview">i + 1</span><script type="math/tex">i + 1</script></span> does not exist. At these points we have to implement the boundary conditions, which can be done in many different ways.</p>
<p>Let us assume that the solution f is a symmetric function about <span><span class="MathJax_Preview">x = 0</span><script type="math/tex">x = 0</script></span>, in which case we can restrict the analysis to positive x and impose a Neuman condition at the origin,</p>
<div>
<div class="MathJax_Preview">
\partial_{x} f=0 \quad \text { at } x=0
</div>
<script type="math/tex; mode=display">
\partial_{x} f=0 \quad \text { at } x=0
</script>
</div>
<p>The two grid points <span><span class="MathJax_Preview">x_0</span><script type="math/tex">x_0</script></span> and <span><span class="MathJax_Preview">x_1</span><script type="math/tex">x_1</script></span> then bracket the boundary point <span><span class="MathJax_Preview">x_{min} = 0</span><script type="math/tex">x_{min} = 0</script></span> symmetrically. We can then write the boundary condition as</p>
<div>
<div class="MathJax_Preview">
f_{1}=f_{0}
</div>
<script type="math/tex; mode=display">
f_{1}=f_{0}
</script>
</div>
<p>For i = 1 we yields</p>
<div>
<div class="MathJax_Preview">
f_{i+1}-f_{i}=(\Delta x)^{2} s_{i} \quad i=1
</div>
<script type="math/tex; mode=display">
f_{i+1}-f_{i}=(\Delta x)^{2} s_{i} \quad i=1
</script>
</div>
<p>We can use a similar strategy at the upper boundary. Let us also assume that f falls off with <span><span class="MathJax_Preview">1/x</span><script type="math/tex">1/x</script></span> for large x, which results in the Robin boundary condition</p>
<div>
<div class="MathJax_Preview">
\partial_{x}(x f)=0 \quad \text { as } x \rightarrow \infty
</div>
<script type="math/tex; mode=display">
\partial_{x}(x f)=0 \quad \text { as } x \rightarrow \infty
</script>
</div>
<p>With the help of a virtual grid point <span><span class="MathJax_Preview">x_{N + 1}</span><script type="math/tex">x_{N + 1}</script></span> we can write the boundary condition in <span><span class="MathJax_Preview">\Delta x</span><script type="math/tex">\Delta x</script></span> as</p>
<div>
<div class="MathJax_Preview">
f_{N+1}=\frac{x_{N}}{x_{N+1}} f_{N}=\frac{x_{N}}{x_{N}+\Delta x} f_{N}
</div>
<script type="math/tex; mode=display">
f_{N+1}=\frac{x_{N}}{x_{N+1}} f_{N}=\frac{x_{N}}{x_{N}+\Delta x} f_{N}
</script>
</div>
<p>We can again insert this into for i = N and find</p>
<div>
<div class="MathJax_Preview">
\left(\frac{x_{i}}{x_{i}+\Delta x}-2\right) f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=N
</div>
<script type="math/tex; mode=display">
\left(\frac{x_{i}}{x_{i}+\Delta x}-2\right) f_{i}+f_{i-1}=(\Delta x)^{2} s_{i} \quad i=N
</script>
</div>
<p>Elliptic Equations now form a coupled set of N linear equations for the N elements <span><span class="MathJax_Preview">f_i</span><script type="math/tex">f_i</script></span> that we can write as</p>
<div>
<div class="MathJax_Preview">
\left( \begin{array}{ccccccc}{-1} &amp; {1} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\ {1} &amp; {-2} &amp; {1} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\ {0} &amp; {\ddots} &amp; {\ddots} &amp; {\ddots} &amp; {0} &amp; {0} &amp; {0} \\ {0} &amp; {0} &amp; {1} &amp; {-2} &amp; {1} &amp; {0} &amp; {0} \\ {0} &amp; {0} &amp; {0} &amp; {\ddots} &amp; {\ddots} &amp; {\ddots} &amp; {0} \\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {1} &amp; {-2} &amp; {1} \\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {1} &amp; {x_{N} /\left(x_{N}+\Delta x\right)-2}\end{array}\right) \cdot \left( \begin{array}{c}{f_{1}} \\ {f_{2}} \\ {\vdots} \\ {f_{i}} \\ {\vdots} \\ {f_{N-1}} \\ {f_{N}}\end{array}\right)=(\Delta x)^{2} \left( \begin{array}{c}{s_{1}} \\ {s_{2}} \\ {\vdots} \\ {s_{i}} \\ {\vdots} \\ {s_{N-1}} \\ {s_{N}}\end{array}\right)
</div>
<script type="math/tex; mode=display">
\left( \begin{array}{ccccccc}{-1} & {1} & {0} & {0} & {0} & {0} & {0} \\ {1} & {-2} & {1} & {0} & {0} & {0} & {0} \\ {0} & {\ddots} & {\ddots} & {\ddots} & {0} & {0} & {0} \\ {0} & {0} & {1} & {-2} & {1} & {0} & {0} \\ {0} & {0} & {0} & {\ddots} & {\ddots} & {\ddots} & {0} \\ {0} & {0} & {0} & {0} & {1} & {-2} & {1} \\ {0} & {0} & {0} & {0} & {0} & {1} & {x_{N} /\left(x_{N}+\Delta x\right)-2}\end{array}\right) \cdot \left( \begin{array}{c}{f_{1}} \\ {f_{2}} \\ {\vdots} \\ {f_{i}} \\ {\vdots} \\ {f_{N-1}} \\ {f_{N}}\end{array}\right)=(\Delta x)^{2} \left( \begin{array}{c}{s_{1}} \\ {s_{2}} \\ {\vdots} \\ {s_{i}} \\ {\vdots} \\ {s_{N-1}} \\ {s_{N}}\end{array}\right)
</script>
</div>
<p>or, in a more compact form,</p>
<div>
<div class="MathJax_Preview">
\mathbf{A} \cdot \mathbf{f}=(\Delta x)^{2} \mathbf{S}
</div>
<script type="math/tex; mode=display">
\mathbf{A} \cdot \mathbf{f}=(\Delta x)^{2} \mathbf{S}
</script>
</div>
<p>The solution is given by</p>
<div>
<div class="MathJax_Preview">
\mathbf{f}=(\Delta x)^{2} \mathbf{A}^{-1} \cdot \mathbf{S}
</div>
<script type="math/tex; mode=display">
\mathbf{f}=(\Delta x)^{2} \mathbf{A}^{-1} \cdot \mathbf{S}
</script>
</div>
<p>so that we have reduced the problem to inverting an N × N matrix.</p>
<h4 id="hyperbolic-equations">Hyperbolic Equations</h4>
<p>For simplicity it does not contain any source terms, and the the wave speed v is constant.</p>
<div>
<div class="MathJax_Preview">
\partial _ { t } u + v \partial _ { x } u = 0
</div>
<script type="math/tex; mode=display">
\partial _ { t } u + v \partial _ { x } u = 0
</script>
</div>
<p>The equation is satisfied exactly by any function of the form <span><span class="MathJax_Preview">u ( t , x ) = u ( x - v t )</span><script type="math/tex">u ( t , x ) = u ( x - v t )</script></span>. <strong>The equation has a time derivative in addition to the space derivative, and thus requires initial data</strong>.</p>
<p>Inserting both finite-difference representations</p>
<div>
<div class="MathJax_Preview">
\left( \partial _ { x } u \right) _ { j } ^ { n } = \frac { u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right) \\
\left( \partial _ { t } u \right) _ { j } ^ { n } = \frac { u _ { j } ^ { n + 1 } - u _ { j } ^ { n } } { \Delta t } + \mathcal { O } ( \Delta t )
</div>
<script type="math/tex; mode=display">
\left( \partial _ { x } u \right) _ { j } ^ { n } = \frac { u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } } { 2 \Delta x } + \mathcal { O } \left( \Delta x ^ { 2 } \right) \\
\left( \partial _ { t } u \right) _ { j } ^ { n } = \frac { u _ { j } ^ { n + 1 } - u _ { j } ^ { n } } { \Delta t } + \mathcal { O } ( \Delta t )
</script>
</div>
<p>we can solve for <span><span class="MathJax_Preview">u^{n+1}_j</span><script type="math/tex">u^{n+1}_j</script></span> and find</p>
<div>
<div class="MathJax_Preview">
u _ { j } ^ { n + 1 } = u _ { j } ^ { n } - \frac { v } { 2 } \frac { \Delta t } { \Delta x } \left( u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } \right)
</div>
<script type="math/tex; mode=display">
u _ { j } ^ { n + 1 } = u _ { j } ^ { n } - \frac { v } { 2 } \frac { \Delta t } { \Delta x } \left( u _ { j + 1 } ^ { n } - u _ { j - 1 } ^ { n } \right)
</script>
</div>
<p>or reasons that are quite obivous this differencing scheme is called forward-time centered-space, or FTCS.</p>
<p><img alt="" src="../media/15512306516707.jpg" /></p>
<p>It is an example of an explicit scheme, meaning that we can solve for the grid function <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> at the new time level n + 1 directly in terms of function values on the old time level n.</p>
<h5 id="courant-friedrichs-lewy-condition">Courant-Friedrichs-Lewy condition</h5>
<p>Unfortunately, however, FTCS is fairly useless. The equation is satisfied exactly by any function of the form <span><span class="MathJax_Preview">u ( t , x ) = u ( x - v t )</span><script type="math/tex">u ( t , x ) = u ( x - v t )</script></span>. we can write the solution <span><span class="MathJax_Preview">u ( t , x )</span><script type="math/tex">u ( t , x )</script></span> to our continuum hyperbolic differential equation as a superposition of eigenmodes <span><span class="MathJax_Preview">e^{i(\omega t+k x)}</span><script type="math/tex">e^{i(\omega t+k x)}</script></span>. Here k is a spatial wave number.</p>
<p>A real <span><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span>, for which <span><span class="MathJax_Preview">e^{i \omega t}</span><script type="math/tex">e^{i \omega t}</script></span> has a magnitude of unity, yields sinusoidally oscillating modes, while the existence of a complex piece in <span><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> leads to exponentially growing or damping modes. In the case of exponential growth, the magnitude of <span><span class="MathJax_Preview">e^{i \omega t}</span><script type="math/tex">e^{i \omega t}</script></span> will exceed unity.</p>
<p>We can perform a similar spectral analysis of the finite difference equation. Write the eigenmode for <span><span class="MathJax_Preview">u_{j}^{n}</span><script type="math/tex">u_{j}^{n}</script></span> as</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n}=\xi^{n} e^{i k(j \Delta x)}
</div>
<script type="math/tex; mode=display">
u_{j}^{n}=\xi^{n} e^{i k(j \Delta x)}
</script>
</div>
<p>Here the quantity <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> plays the role of <span><span class="MathJax_Preview">e^{i \omega \Delta t}</span><script type="math/tex">e^{i \omega \Delta t}</script></span> and is called the amplification factor:</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n}=\xi u_{j}^{n-1} = \xi^{2} u_{j}^{n-2} \ldots=\xi^{n} u_{j}^{0}
</div>
<script type="math/tex; mode=display">
u_{j}^{n}=\xi u_{j}^{n-1} = \xi^{2} u_{j}^{n-2} \ldots=\xi^{n} u_{j}^{0}
</script>
</div>
<p>For the scheme to be stable, the magnitude <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> must be smaller or equal to unity for all k,</p>
<div>
<div class="MathJax_Preview">
|\xi(k)| \leq 1
</div>
<script type="math/tex; mode=display">
|\xi(k)| \leq 1
</script>
</div>
<p>To perform a von Neumann stability anaylsis of the FTCS scheme</p>
<div>
<div class="MathJax_Preview">
\xi(k)=1-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</div>
<script type="math/tex; mode=display">
\xi(k)=1-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</script>
</div>
<p><strong>the magnitude of <span><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> is greater than unity for all k, indicating that this scheme is unstable.</strong> In fact, we have <span><span class="MathJax_Preview">|\xi|&gt;1</span><script type="math/tex">|\xi|>1</script></span> independently of our choice for <span><span class="MathJax_Preview">\Delta x</span><script type="math/tex">\Delta x</script></span> and <span><span class="MathJax_Preview">\Delta t</span><script type="math/tex">\Delta t</script></span>, which makes this scheme unconditionally unstable. That is bad.</p>
<p>The good news is that there are several ways of fixing this problem.</p>
<p>For example</p>
<p>We could replace the term <span><span class="MathJax_Preview">u_{j}^{n}</span><script type="math/tex">u_{j}^{n}</script></span> by the spatial average <span><span class="MathJax_Preview">\left(u_{j+1}^{n}+u_{j-1}^{n}\right) / 2</span><script type="math/tex">\left(u_{j+1}^{n}+u_{j-1}^{n}\right) / 2</script></span>.</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=\frac{1}{2}\left(u_{j+1}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=\frac{1}{2}\left(u_{j+1}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>a von Neumann analysis results in the amplification factor</p>
<div>
<div class="MathJax_Preview">
\xi=\cos k \Delta x-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</div>
<script type="math/tex; mode=display">
\xi=\cos k \Delta x-i \frac{v \Delta t}{\Delta x} \sin k \Delta x
</script>
</div>
<p>The von Neumann stability criterion then implies that we must have</p>
<div>
<div class="MathJax_Preview">
\frac { | v | \Delta t } { \Delta x } \leq 1
</div>
<script type="math/tex; mode=display">
\frac { | v | \Delta t } { \Delta x } \leq 1
</script>
</div>
<p>The Courant condition states that the the grid point <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> at the new time level n+1 has to reside inside the domain of determinacy of the interval spanned by the finite difference stencil at the time level n. This makes intuitive sense: if <span><span class="MathJax_Preview">u _ { j } ^ { n + 1 }</span><script type="math/tex">u _ { j } ^ { n + 1 }</script></span> were outside this domain, its physical specification would require more information about the past than we are providing numerically, which may trigger an instability.</p>
<p><img alt="" src="../media/15512306641761.jpg" /></p>
<p>Recalling that v represents the speed of a characteristic, we may interpret the Courant condition in terms of the domain of determinacy.</p>
<p><strong>It seems somewhat like a miracle that simply replacing a grid function by a local average manages to change the numerical scheme from unconditionally unstable to conditionally stable.</strong></p>
<p>This change can be interpreted in very physical terms</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n}+\frac{1}{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n}+\frac{1}{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>or</p>
<div>
<div class="MathJax_Preview">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \frac{u_{j+1}^{n}-u_{j-1}^{n}}{2 \Delta x}+\frac{(\Delta x)^{2}}{2 \Delta t} \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{(\Delta x)^{2}}
</div>
<script type="math/tex; mode=display">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \frac{u_{j+1}^{n}-u_{j-1}^{n}}{2 \Delta x}+\frac{(\Delta x)^{2}}{2 \Delta t} \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{(\Delta x)^{2}}
</script>
</div>
<p>But equation is a finite-difference representation of the differential equation</p>
<div>
<div class="MathJax_Preview">
\partial_{t} u+v \partial_{x} u=D \partial_{x}^{2} u
</div>
<script type="math/tex; mode=display">
\partial_{t} u+v \partial_{x} u=D \partial_{x}^{2} u
</script>
</div>
<p>where the term on the right-hand side is essentially a diffusion term, with parameter <span><span class="MathJax_Preview">D=(\Delta x)^{2} /(2 \Delta t)</span><script type="math/tex">D=(\Delta x)^{2} /(2 \Delta t)</script></span> serving as a constant coefficient of diffusion.</p>
<p>This feature implies the amplitude of any wave will decrease spuriously with time as it propagates. A related effect is anomalous dispersion, an additional price we pay for stablity in the Lax scheme and many other finite-difference schemes for hyperbolic systems.</p>
<h5 id="other-scheme">Other scheme</h5>
<p>Lax scheme</p>
<hr />
<p>There are a number of other ways of constructing stable finite difference schemes for the model equation. A popular alternative to the Lax scheme is upwind differencing</p>
<div>
<div class="MathJax_Preview">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \left\{\begin{array}{l}{\frac{u_{j}^{n}-u_{j-1}^{n}}{\Delta x},} &amp; {v&gt;0} \\ {\frac{u_{j+1}^{n}-u_{j}^{n}}{\Delta x},} &amp; {v&gt;0}\end{array}\right.
</div>
<script type="math/tex; mode=display">
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}=-v \left\{\begin{array}{l}{\frac{u_{j}^{n}-u_{j-1}^{n}}{\Delta x},} & {v>0} \\ {\frac{u_{j+1}^{n}-u_{j}^{n}}{\Delta x},} & {v>0}\end{array}\right.
</script>
</div>
<p>This scheme borrows its name from the fact that for a wave with <span><span class="MathJax_Preview">v&gt;0</span><script type="math/tex">v>0</script></span>, that travels “to the right”, say, the new grid-point <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> is affected only by the “upwind” grid-points to its left, i.e. that lie in the region through which the wave travels before reaching <span><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span>.</p>
<p>three-level scheme</p>
<hr />
<p>It would be desirable, however, to have a scheme that is second order in both in space and time. One way to construct such a code is to abandon two-level schemes, and instead consider a three-level scheme. We can then construct centered derivatives both for the time-derivative,</p>
<div>
<div class="MathJax_Preview">
\left(\partial_{t} u\right)_{j}^{n}=\frac{u_{j}^{n+1}-u_{j}^{n-1}}{2 \Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
</div>
<script type="math/tex; mode=display">
\left(\partial_{t} u\right)_{j}^{n}=\frac{u_{j}^{n+1}-u_{j}^{n-1}}{2 \Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
</script>
</div>
<p>the leap-frog scheme,</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n-1}-v \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n-1}-v \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>Some researchers prefer two-level schemes over three-level schemes because three level schemes require initial data on two different time levels, which can be somewhat awkward.</p>
<p><img alt="-w619" src="../media/15532637365521.jpg" /></p>
<p>The leap-frog scheme has the additional disadvantage that, it only connects fields of the same color. “Black” gridpoints can therefore evolve completely independently of “white” gridpoints, and the two sets of grid points may drift apart as numerical error accumulates differently for the two sets of points. If necessary, this problem can be solved by artificially adding a very small viscous term that links the two sets together. Once these potential issues are resolved, leap-frog is a very simple, accurate and powerful method.</p>
<p>implicit scheme</p>
<hr />
<p>Yet another way of constructing a stable two-level scheme is to use backward time differencing instead of forward differencing. This approach then yields the “backward-time, centered-space” scheme,</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{2} \frac{\Delta t}{\Delta x}\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)
</script>
</div>
<p><img alt="-w600" src="../media/15532642037929.jpg" /></p>
<p>Performing a von Neumann stability analysis we find the amplification factor</p>
<div>
<div class="MathJax_Preview">
|\xi(k)| \leq 1
</div>
<script type="math/tex; mode=display">
|\xi(k)| \leq 1
</script>
</div>
<p>for all values of <span><span class="MathJax_Preview">\Delta t</span><script type="math/tex">\Delta t</script></span>. This finding means that this scheme is unconditionally stable. <strong>The size of the stepsize ∆t is no longer restricted by stability, and instead is limited only by accuracy requirements.</strong> this property is even more important for parabolic equations.</p>
<p>The disadvantage of the backward differencing scheme is that we can no longer solve for the new grid function <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> at the new time <span><span class="MathJax_Preview">t^{n+1}</span><script type="math/tex">t^{n+1}</script></span> explicitly in terms of old grid functions at <span><span class="MathJax_Preview">t^{n}</span><script type="math/tex">t^{n}</script></span> alone. Instead, now couples <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> with the its closest neighbors <span><span class="MathJax_Preview">u_{j+1}^{n+1}</span><script type="math/tex">u_{j+1}^{n+1}</script></span> and <span><span class="MathJax_Preview">u_{j-1}^{n+1}</span><script type="math/tex">u_{j-1}^{n+1}</script></span>. This coupling provides an implicit linear relation between the new grid functions, and is therefore an example of an implicit finite-differencing scheme. We can no longer sweep through the grid and update one point at a time; instead <strong>we now have to solve for all grid points simultaneously</strong>. Writing equation at all interior grid points, and then taking into account the boundary conditions, leads to a system of equations quite similar to elliptic equations.</p>
<p>Crank-Nicholson scheme</p>
<hr />
<p>A second-order scheme would be time-centered, meaning that we should estimate the the time derivative at the mid-point between the two time levels n and n + 1,</p>
<p>Implementing time-centering means that we also have to evaluate the space derivative at this same midpoint <span><span class="MathJax_Preview">n+1/2</span><script type="math/tex">n+1/2</script></span>  which we can do by averaging between the values at n and n+1.</p>
<div>
<div class="MathJax_Preview">
\left(\partial_{t} u\right)_{j}^{n+1 / 2}=\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
</div>
<script type="math/tex; mode=display">
\left(\partial_{t} u\right)_{j}^{n+1 / 2}=\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}+\mathcal{O}\left(\Delta t^{2}\right)
</script>
</div>
<p>This approach yields the Crank-Nicholson scheme</p>
<div>
<div class="MathJax_Preview">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{4} \frac{\Delta t}{\Delta x}\left(\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right)\right)
</div>
<script type="math/tex; mode=display">
u_{j}^{n+1}=u_{j}^{n}-\frac{v}{4} \frac{\Delta t}{\Delta x}\left(\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right)\right)
</script>
</div>
<p><img alt="-w696" src="../media/15532650020363.jpg" /></p>
<p>Crank-Nicholson is second order in both space and time, and shows that it is unconditionally stable.</p>
<p>The iterative Crank-Nicholson scheme uses a predictor-corrector approach. In the predictor step we predict the new values <span><span class="MathJax_Preview">u_{j}^{n+1}</span><script type="math/tex">u_{j}^{n+1}</script></span> by using the fully explicit FTCS scheme.</p>
<div>
<div class="MathJax_Preview">
^{(1)} u_{j}^{n+1}=u_{j}^{n}-\frac{v \Delta t}{2 \Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</div>
<script type="math/tex; mode=display">
^{(1)} u_{j}^{n+1}=u_{j}^{n}-\frac{v \Delta t}{2 \Delta x}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
</script>
</div>
<p>which, as we have seen above, would be unconditionally unstable by itself. In a subsequent corrector step we use these predicted values <span><span class="MathJax_Preview">^{(1)}{u_{j}}^{n+1}</span><script type="math/tex">^{(1)}{u_{j}}^{n+1}</script></span> together with the <span><span class="MathJax_Preview">u_{j}^{n}</span><script type="math/tex">u_{j}^{n}</script></span> to obtain a time-centered approximation for the spatial derivative on the right-hand side of equation. This step yields the corrected values of the grid function,</p>
<div>
<div class="MathJax_Preview">
^{(2)} u_{j}^{n+1}=u_{j}^{n} - \frac{v \Delta t}{4 \Delta x} \left(\left( ^{(1)} u_{j+1}^{n+1} - ^{(1)} u_{j-1}^{n+1}\right)\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right) )
</div>
<script type="math/tex; mode=display">
^{(2)} u_{j}^{n+1}=u_{j}^{n} - \frac{v \Delta t}{4 \Delta x} \left(\left( ^{(1)} u_{j+1}^{n+1} - ^{(1)} u_{j-1}^{n+1}\right)\right)+\left(u_{j+1}^{n}-u_{j-1}^{n}\right) )
</script>
</div>
<p>The corrector step can be repeated an arbitrary number times N, always using the previous values <span><span class="MathJax_Preview">^{(N-1)} u_{j}^{n+1}</span><script type="math/tex">^{(N-1)} u_{j}^{n+1}</script></span> on the right-hand side to find new corrected values <span><span class="MathJax_Preview">^{(N)} u_{j}^{n+1}</span><script type="math/tex">^{(N)} u_{j}^{n+1}</script></span>.</p>
<p>The iterative Crank-Nicholson scheme is an explicit two-level scheme that is second order in both space and time. Since this form is very similar to the 3 + 1 equations and related formulations, the iterative Crank-Nicholson scheme has often been used in numerical relativity simulations.</p>
<h5 id="method-of-lines">Method of Lines</h5>
<p>A popular alternative to these complete finite difference schemes is therefore the method of lines (or MOL for short).</p>
<p><strong>The basic idea of the method of lines is to finite difference the space derivatives only.</strong> Now we introduce a spatial grid only, at least for now, so that the function values at these gridpoint, <span><span class="MathJax_Preview">u_{j}(t)=u\left(t, x_{j}\right)</span><script type="math/tex">u_{j}(t)=u\left(t, x_{j}\right)</script></span>, remain functions of time. As a result, our partial differential equation for <span><span class="MathJax_Preview">u(t, x)</span><script type="math/tex">u(t, x)</script></span> becomes a set of ordinary differential equations for the grid values <span><span class="MathJax_Preview">u_{j}(t)</span><script type="math/tex">u_{j}(t)</script></span>. The next question is how to integrate the ordinary differential equations. The appealing feature of the method of lines, however, is that we can use any method for the integration of the ordinary differential equations that we like. In fact, many such methods, including very efficient, high-order methods, are precoded and readily available. One such algorithm is the ever-popular Runge-Kutta method.</p>
<p>To implement, say, a fourth-order scheme for our model</p>
<div>
<div class="MathJax_Preview">
\partial_{t} u+v \partial_{x} u=0
</div>
<script type="math/tex; mode=display">
\partial_{t} u+v \partial_{x} u=0
</script>
</div>
<p>we could adopt the fourth-order differencing stencil to replace the spatial derivative, yielding</p>
<div>
<div class="MathJax_Preview">
\frac{d u_{j}}{d t}=-\frac{v}{12 \Delta x}\left(u_{i-2}-8 u_{j-1}+8 u_{j+1}-u_{i+2}\right)
</div>
<script type="math/tex; mode=display">
\frac{d u_{j}}{d t}=-\frac{v}{12 \Delta x}\left(u_{i-2}-8 u_{j-1}+8 u_{j+1}-u_{i+2}\right)
</script>
</div>
<p>and then integrate this set of ordinary differential equations with a fourth-order Runge-Kutta method.</p>
<h3 id="ghost-zones">Ghost Zones</h3>
<p>Cactus is based upon a distributed computing paradigm. That is, the problem domain is split into blocks, each of which is assigned to a processor.</p>
<p>Consider the 1-D wave equation</p>
<div>
<div class="MathJax_Preview">
\frac{\partial^{2} \phi}{\partial t^{2}}=\frac{\partial^{2} \phi}{\partial x^{2}}
</div>
<script type="math/tex; mode=display">
\frac{\partial^{2} \phi}{\partial t^{2}}=\frac{\partial^{2} \phi}{\partial x^{2}}
</script>
</div>
<p>To solve this by partial differences, one discretises the derivatives to get an equation relating the solution
at different times.</p>
<div>
<div class="MathJax_Preview">
\phi(t+\Delta t, x)-2 \phi(t, x)+\phi(t-\Delta t, x)=\frac{\Delta t^{2}}{\Delta x^{2}}\{\phi(t, x+\Delta x)-2 \phi(t, x)+\phi(t, x-\Delta x)\}
</div>
<script type="math/tex; mode=display">
\phi(t+\Delta t, x)-2 \phi(t, x)+\phi(t-\Delta t, x)=\frac{\Delta t^{2}}{\Delta x^{2}}\{\phi(t, x+\Delta x)-2 \phi(t, x)+\phi(t, x-\Delta x)\}
</script>
</div>
<p>On examination, you can see that to generate the data at the point <span><span class="MathJax_Preview">(t+\Delta t, x)</span><script type="math/tex">(t+\Delta t, x)</script></span> we need data from the four points <span><span class="MathJax_Preview">(t, x)</span><script type="math/tex">(t, x)</script></span>, <span><span class="MathJax_Preview">(t -\Delta t, x)</span><script type="math/tex">(t -\Delta t, x)</script></span>, <span><span class="MathJax_Preview">(t, x + \Delta x)</span><script type="math/tex">(t, x + \Delta x)</script></span> and <span><span class="MathJax_Preview">(t, x- \Delta x)</span><script type="math/tex">(t, x- \Delta x)</script></span> only.</p>
<p>Now, if you evolve the above scheme, it becomes apparent that at each iteration the number of grid points you can evolve decreases by one at each edge.</p>
<p><img alt="-w816" src="../media/15528964483464.jpg" /></p>
<p>At the outer boundary of the physical domain, the data for the boundary point can be generated by the boundary conditions, however, at internal boundaries, the data has to be copied from the adjacent processor.</p>
<p>It would be inefficient to copy each point individually, so instead, a number of ghostzones are created at the internal boundaries. A ghostzone consists of a copy of the whole plane (in 3D, line in 2D, point in 1D) of the data from the adjacent processor.</p>
<p><img alt="-w785" src="../media/15528964642193.jpg" /></p>
<p>Once the data has been evolved one step, the data in the ghostzones can be exchanged (or synchronised) between processors in one fell swoop before the next evolution step.</p>
<h3 id="mesh-refinement">Mesh Refinement</h3>
<p>Many current numerical relativity codes use a uniform grid spacing to cover the entire spatial domain. Given that computational resources are limited, so that we can afford only a finite number of gridpoints, such a “unigrid” implementation may pose a problem.</p>
<p>Imagine, for concreteness, a simulation of a strong-field gravitational wave source, like a compact binary containing neutron stars or black holes. On the one hand we have to resolve these sources well, so as to minimize truncation error in the strong-field region. On the other hand, the grid must extend into the weak-field region at large distances from the sources, so as to minimize error from the outer boundaries and to enable us to extract the emitted gravitational radiation accurately.</p>
<p><img alt="-w1062" src="../media/15533057187758.jpg" /></p>
<p>A very promising alternative is mesh refinement, which has been widely developed and used in the computational fluid dynamics community and is becoming increasingly popular in numerical relativity.</p>
<p>The basic idea underlying mesh refinement techniques is to perform the simulation not on one numerical grid, but on several, as in the multigrid methods.</p>
<p><img alt="-w731" src="../media/15533068726818.jpg" /></p>
<p>In multigrid methods, the numerical solution is computed on a hierarchy of computational grids with increasing grid resolution. The finer grids may or may not cover all the physical space that is covered by the coarser grids. The numerical solution is then computed by completing sweeps through the grid hierarchy. </p>
<p>The coarse grid is sufficiently small so that we can compute a solution with a direct solver. This provides the “global” features of the solution, albeit on a coarse grid and hence with a large local truncation error. We then interpolate this approximate solution to the next finer grid. This interpolation from a coarser grid to a finer grid is called a “prolongation”, and we point out that the details of this interpolation depend on whether the grid is cell-centered or vertex-centered. </p>
<blockquote>
<p>In the mathematical field of numerical analysis, interpolation is a method of constructing new data points within the range of a discrete set of known data points.</p>
</blockquote>
<p>On the finer grid we can then apply a relaxation method, for example a Gauss-Seidel sweep. While this method is too slow to solve the problem globally, as we have discussed above, it is very well suited to improve the solution locally. This step is often called a “smoothing sweep”. </p>
<p>After this smoothing sweep the solution can be prolonged to the next finer grid, where the procedure is repeated. Once we have smoothed the solution on the finest grid, we start ascending back to coarser grids. The interpolation from a finer grid to a coarser grid is called a “restriction”. </p>
<p>The coarser grids now “learn” from the finer grids by comparing their last solution with the one that comes back from a finer grid. This comparison provides an estimate for the local truncation error, which can be accounted for with the help of an artificial source term. On each grid we again perform smoothing sweeps, again improving the solution because we inherit the smaller truncation error from the finer grids. These sweeps through the grid hierarchy can be repeated until the solution has converged to a pre-determined accuracy.</p>
<p>Typically, the gridspacing on the finer grid is half that on the next coarser grid, but clearly other refinement factors can be chosen. The hierarchy can be extended, and typical mesh refinement applications employ multiple refinement levels.</p>
<p>Two versions of mesh refinement can be implemented. In the simpler version, called fixed mesh refinement or FMR, it is assumed that the refined grids will be needed only at known locations in space that remain fixed throughout the simulation.</p>
<p>The situation is more complicated for objects that are moving, as is the case for a coalescing binary star system. In this case we do not know a priori the trajectories of the companion stars, hence do not know which regions need refining. Moreover, these regions will be changing as the system evolves and the stars move. Clearly, we would like to move the refined grids with the stars. Such an approach, whereby the grid is relocated during the simulation to give optimal resolution at each time step, is called adaptive mesh refinement or AMR.</p>
<h4 id="fixed-mesh-refinement">Fixed Mesh Refinement</h4>
<p>A standard way of solving partial differential equations are finite differences on a regular grid. This is also called unigrid. Such an application discretises its problem space onto a single, rectangular grid which has everywhere the same grid spacing. Increasing the resolution in a unigrid application is somewhat expensive. For example, increasing the resolution by a factor of two requires a factor of eight more storage in three dimensions. Most applications need the high resolution only in a part of the simulation domain.</p>
<p>Instead of only one grid, there are several grids or grid patches with different resolutions. The coarsest grid usually encloses the whole simulation domain. Successively finer grids overlay the coarse grid at those locations where a higher resolutions is needed. The coarser grids provide boundary conditions to the finer grid through interpolation.</p>
<p>Instead of updating only one grid, the application has to update all grids. The usual approach is to first take a step on the coarsest grid, and then recursively take several smaller steps on the finer grids. The Courant criterion requires that the step sizes on the finer grids be smaller than on the coarse grid. The boundary values for the finer grids are found through interpolation in space and time from the coarser grid. In the end, the information on the finer grids is injected into the coarse grids.</p>
<h4 id="adaptive-mesh-refinement-amr">Adaptive mesh refinement (AMR)</h4>
<p>For well behaved problems a grid of uniform mesh spacing (in each of the coordinate directions) gives satisfactory results. However, there are classes of problems where the solution is more difficult to estimate in some regions (perhaps due to discontinuities, steep gradients, shocks, etc.) than in others. One could use a uniform grid having a <strong>spacing fine enough so that the local errors estimated in these difficult regions are acceptable</strong>. But this approach is computationally extremely costly.</p>
<p>In numerical analysis, <strong>adaptive mesh refinement (AMR)</strong> is a method of <strong>adapting the accuracy of a solution within certain sensitive or turbulent regions</strong> of simulation, dynamically and during the time the solution is being calculated. The use of AMR has since then proved of broad use and has been used in studying turbulence problems in hydrodynamics as well as in the study of large scale structures in astrophysics。</p>
<p><img alt="" src="../media/15523870802272.jpg" /></p>
<ol>
<li>Start with a coarse grid</li>
<li>Identify regions that need finer resolution</li>
<li>Superimpose finer sub-grids only on those regions </li>
<li>Finer and finer subgrids are added recursively until either a given maximum level of refinement is reached or the local truncation error has dropped below the desired level</li>
</ol>
<p><img alt="" src="../media/15523870938244.jpg" /></p>
<p>The complete computational grid consists of a collection of blocks with different physical cell sizes, which are related to each other in a hierarchical fashion using a tree data structure. The blocks at the root of the tree have the largest cells, while their children have smaller cells and are said to be refined. </p>
<p><img alt="" src="../media/15523871071222.jpg" /></p>
<p>Three rules govern the establishment of refined child blocks.</p>
<ol>
<li>a refined child block must be one-half as large as its parent block in each spatial dimension. </li>
<li>a block’s children must be nested; i.e., the child blocks must fit within their parent block and cannot overlap one another, and the complete set of children of a block must fill its volume. Thus, in d dimensions a given block has either zero or <span><span class="MathJax_Preview">2^d</span><script type="math/tex">2^d</script></span> children.</li>
<li>blocks which share a common border may not differ from each other by more than one level of refinement.</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../SP/Introduction/" class="btn btn-neutral float-right" title="Introduction">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Matter Sources/" class="btn btn-neutral" title="Matter Sources"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/YuLiumt/Einstein-Toolkit/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../Matter Sources/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../SP/Introduction/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
